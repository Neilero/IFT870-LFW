{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IFT870 - TP4\n",
    "\n",
    "Auteur : Aurélien Vauthier (19 126 456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import adjusted_rand_score, make_scorer, silhouette_score\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from scipy.spatial.distance import cdist, pdist, squareform\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# %matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement d'un ensemble de données de faces de personnages connus\n",
    "from sklearn.datasets import fetch_lfw_people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = fetch_lfw_people(min_faces_per_person=20, resize=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format des images et nombres de clusters\n",
    "print(\"Format des images: {}\".format(faces.images.shape))\n",
    "print(\"Nombre de classes: {}\".format(len(faces.target_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nombre de données par cluster\n",
    "nombres = np.bincount(faces.target)\n",
    "for i, (nb, nom) in enumerate(zip(nombres, faces.target_names)):\n",
    "    print(\"{0:25} {1:3}\".format(nom, nb), end='   ')\n",
    "    if (i + 1) % 3 == 0:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des 10 premières faces\n",
    "fig, axes = plt.subplots(2, 5, figsize=(10, 6),\n",
    "                         subplot_kw={'xticks': (), 'yticks': ()})\n",
    "for nom, image, ax in zip(faces.target, faces.images, axes.ravel()):\n",
    "    ax.imshow(image)\n",
    "    ax.set_title(faces.target_names[nom])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Pour commencer, les données ne sont pas équilibrées car certains personnages sont beaucoup\n",
    "plus représentés que d’autres. Pour pallier à cela, filter les données pour ne conserver que\n",
    "40 visages au maximum par personne.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data array to DataFrame and append targets\n",
    "data = pd.DataFrame(faces.data)\n",
    "data[\"target\"] = faces.target\n",
    "\n",
    "# keep the first 40 data for each target\n",
    "data = data.groupby(\"target\").head(40)\n",
    "\n",
    "# show results\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ensuite, appliquer une réduction de la dimension à 100 composantes et une normalisation\n",
    "en utilisant le modèle `PCA()` de `sklearn` avec les options `whiten=True` et `random_state=0`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(100, whiten=True, random_state=0)\n",
    "\n",
    "X = pca.fit_transform(data.drop(\"target\", axis=1))\n",
    "y = data[\"target\"]\n",
    "data = pd.concat([pd.DataFrame(X, index=y.index), y], axis=1)\n",
    "\n",
    "# show results\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse avec K-Means\n",
    "\n",
    "*Implémenter la méthode du coude (Elbow method) pour essayer de déterminer un\n",
    "nombre de clusters optimals dans l’ensemble suivant [40, 45, 50, 55, 60, ..., 80] sans\n",
    "utiliser les données réelles (noms associés aux images). La mesure de score à utiliser\n",
    "pour tout nombre de clusters k est la suivante : moyenne des distances euclidiennes des\n",
    "données à leur plus proche centre de cluster pour le modèle à k clusters. Analyser le\n",
    "résultat et donner vos conclusions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.geeksforgeeks.org/elbow-method-for-optimal-value-of-k-in-kmeans/\n",
    "Ks = range(40, 81, 5)\n",
    "mean_min_dists = []\n",
    "\n",
    "for k in tqdm(Ks, desc=\"Computing mean of min distances to closest centroid...\"):\n",
    "    kmean = KMeans(n_clusters=k, n_jobs=-1, random_state=0)\n",
    "    kmean.fit(X)\n",
    "\n",
    "    distances = cdist(X, kmean.cluster_centers_, \"euclidean\")\n",
    "    mean_min_dist = np.mean(np.min(distances, axis=1))\n",
    "    mean_min_dists.append(mean_min_dist)\n",
    "\n",
    "plt.plot(Ks, mean_min_dists, 'bx-')\n",
    "plt.xlabel(\"Nombre de cluster K\")\n",
    "plt.ylabel(\"Moyenne des distances\")\n",
    "plt.title(\"Moyennes des distances des données à leur plus proche centre par rapport au nombre de cluster\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'après les résultats du graphique ci-dessus, nous pouvons observer deux faible \"coudes\" autour des valeurs 55/60 et de\n",
    "la valeur 75. Nous pourrions donc tenter de regarder les clusters formés à ces valeurs manuellement pour vérifier leur\n",
    "potentiel.\n",
    "\n",
    "Nous pouvons expliquer ces faibles résultats par la\n",
    "[malédiction de la dimensionnalité](https://fr.wikipedia.org/wiki/Fl%C3%A9au_de_la_dimension). En effet, on peut\n",
    "constater que lorsque le nombre de dimensions augmente la distance entre les points tend à s'uniformiser. Cela rend\n",
    "ainsi les calculs de distances moins performant, moins significatif."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Appliquer une approche de validation croisée en divisant les données en 10 parties et en\n",
    "utilisant les données réelles et le score `Adjusted_Rand_Index` (ARI) pour déterminer\n",
    "un nombre de clusters optimal dans l’ensemble [40, 45, 50, 55, 60, ..., 80]. Analyser le\n",
    "résultat et donner vos conclusions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_jobs=-1, random_state=0)\n",
    "param_grid = {\n",
    "    \"n_clusters\": Ks\n",
    "}\n",
    "\n",
    "# convert metric to scorer\n",
    "scorer_ARI = make_scorer(adjusted_rand_score)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "best_kmean = GridSearchCV(model, param_grid, cv=cv, scoring=scorer_ARI, n_jobs=-1, verbose=1)\n",
    "best_kmean.fit(X, y)\n",
    "\n",
    "print(f\"Best k fond : {best_kmean.best_params_['n_clusters']}\")\n",
    "\n",
    "plt.plot(Ks, best_kmean.cv_results_[\"mean_test_score\"], 'bx-')\n",
    "plt.xlabel(\"Nombre de cluster K\")\n",
    "plt.ylabel(\"Moyenne des scores\")\n",
    "plt.title(\"Moyennes des scores en fonction du nombre de cluster\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons remarquer que, si on retire le paramètre `random_state`, les résultats changent à chaque exécution et ces\n",
    "résultats sont parfois oscillants, rendant la précision de la validation croisée douteuse.\n",
    "\n",
    "Nous pouvons aussi noter que les potentiels meilleurs clustering sont cette fois-ci obtenus pour des valeurs de\n",
    "`n_clusters` à 40, 60 et 80 (ce dernier étant d'après notre mesure de score le meilleur clustering). Là encore, une\n",
    "vérification manuelle de la sémantique des clusters formés serait nécessaire pour les évaluer plus précisément.\n",
    "\n",
    "Ces résultats semblent donc confirmer la difficulté de classifier ces données. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse avec DBSCAN\n",
    "\n",
    "*Utiliser le coéfficient de silhouette pour déterminer les meilleurs valeurs de paramètres\n",
    "(nombre minimum d’éléments dans un cluster `min_samples`, et rayon du voisinage autour\n",
    "de chaque donnée `eps`) pour la méthode DBSCAN avec `min_samples` dans l’intervalle\n",
    "[1, ..., 10] et eps dans l’intervalle [5, ..., 15]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-compute pair-wise distances\n",
    "distances = squareform(pdist(X))\n",
    "\n",
    "eps_list = range(5, 16)\n",
    "min_samples_list = range(11)\n",
    "\n",
    "scores = []\n",
    "n_clusters = []\n",
    "best_dbscan = None\n",
    "best_dbscan_score = 0\n",
    "best_dbscan_n_cluster = 0\n",
    "best_dbscan_params = None\n",
    "for eps, min_samples in tqdm(product(eps_list, min_samples_list), total=len(eps_list)*len(min_samples_list),\n",
    "                             desc=\"Searching best eps and min_samples for DBSCAN\"):\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples, n_jobs=-1)\n",
    "    predict = dbscan.fit_predict(X, y)\n",
    "\n",
    "    score = silhouette_score(distances, predict) if len(np.unique(predict)) > 1 else 0  # if only noise found, score = 0\n",
    "    n_cluster = len(np.unique(dbscan.labels_)) - (1 if -1 in dbscan.labels_ else 0)   # we don't count noise\n",
    "\n",
    "    scores.append(score)\n",
    "    n_clusters.append(n_cluster)\n",
    "    if score > best_dbscan_score:\n",
    "        best_dbscan_score = score\n",
    "        best_dbscan = dbscan\n",
    "        best_dbscan_params = (eps, min_samples)\n",
    "        best_dbscan_n_cluster = n_cluster\n",
    "\n",
    "print(f\"Best eps = {best_dbscan_params[0]}, best min_samples = {best_dbscan_params[1]}\")\n",
    "print(f\"Number of cluster(s) for the best model : {best_dbscan_n_cluster}\")\n",
    "\n",
    "def plot_heatmap_of_hyperparam(values, title, annot=False):\n",
    "    values = np.reshape(values, (len(eps_list), len(min_samples_list)))\n",
    "    sns.heatmap(values, xticklabels=min_samples_list, yticklabels=eps_list, annot=annot)\n",
    "    plt.suptitle(title)\n",
    "    plt.xlabel(\"min_samples\")\n",
    "    plt.ylabel(\"eps\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# plot scores heat map\n",
    "plot_heatmap_of_hyperparam(scores, \"Scores en fonction des hyperparamètres\")\n",
    "\n",
    "# plot n_clusters heat map\n",
    "plot_heatmap_of_hyperparam(n_clusters, \"N_clusters en fonction des hyperparamètres\")\n",
    "\n",
    "# plot n_clusters heat map for values less  than 100\n",
    "n_clusters = np.where(np.array(n_clusters) < 100, n_clusters, np.nan)\n",
    "plot_heatmap_of_hyperparam(n_clusters, \"N_clusters (valeurs < 100) en fonction des hyperparamètres\", annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'après les résultats ci-dessus, on peut remarquer que les meilleurs scores de silhouette sont obtenu pour un\n",
    "`eps` élevé et un `min_sample` moyen à élevé. Nous pouvons ainsi déduire de ces résultats que les meilleurs clusters au\n",
    "sens du coefficient de silhouette sont les plus gros. Il semble aussi que les valeurs de score soient plus sensibles aux\n",
    "modifications d'`eps` que de `min_samples`.\n",
    "\n",
    "Cependant, si on observe le nombre de clusters, on s'aperçoit que les clusterings avec les meilleurs scores sont aussi\n",
    "souvent ceux qui ont un nombre de clusters très faible. Cette observation est assez contre-intuitive car on s'attend en\n",
    "général à obtenir $\\sqrt{n}$ clusters, ce qui pour notre cas correspondrait à $\\sqrt{1916} \\approx 44$. Par conséquent,\n",
    "il serait donc intéressant de vérifier les clusterings avec plus de clusters qui pourraient ainsi mieux séparer les\n",
    "données.\n",
    "\n",
    "On peut aussi noter la présence de clustering sans cluster (ou plutôt uniquement composé de bruit) lorsque les valeurs\n",
    "d'`eps` sont faibles et les valeurs de `min_samples` moyennement à élevé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*En fixant le paramètre `min_samples = 3`, appliquer DBSCAN en faisant varier le paramètre\n",
    "`eps` dans l’intervalle [5, ..., 15]. Observer des échantillons d’images des clusters\n",
    "pour chaque rayon dans l’intervalle [5, ..., 15], et tenter de déterminer la signification\n",
    "sémantique des clusterings estimés. Elle peut correspondre à un clustering suivant les\n",
    "personnages, ou suivant d’autres caractéristiques commune comme l’orientation du visage,\n",
    "l’arrière plan, le port de lunette, etc. Lister vos conclusions pour chaque valeur de\n",
    "`eps`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove warning printing more than 20 images\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "\n",
    "def show_cluster_samples(data, X, y, eps):\n",
    "    predict = DBSCAN(eps=eps, min_samples=3, n_jobs=-1).fit_predict(X, y)\n",
    "    data[\"cluster\"] = predict\n",
    "\n",
    "    image_count = 5\n",
    "\n",
    "    for i, (cluster, clusterData) in enumerate(data.groupby(\"cluster\")):\n",
    "        if cluster == -1:\n",
    "            continue\n",
    "\n",
    "        fig, axes = plt.subplots(ncols=image_count, figsize=(10, 3), subplot_kw={'xticks': (), 'yticks': ()})\n",
    "        fig.suptitle(f\"Échantillon d'images du cluster {cluster} (taille réel : {clusterData.shape[0]})\", fontsize=16)\n",
    "\n",
    "        if cluster == 0:\n",
    "            fig.text(0.5, 1, f\"Clusters pour eps={eps}\", horizontalalignment='center', fontsize=20)\n",
    "\n",
    "        for j in range(image_count):\n",
    "            if j >= clusterData.shape[0]:\n",
    "                axes[j].set_visible(False)\n",
    "                continue\n",
    "\n",
    "            row_index = clusterData.index[j]\n",
    "            axes[j].imshow(faces.images[row_index])\n",
    "            axes[j].set_title(faces.target_names[clusterData[\"target\"][row_index]])\n",
    "\n",
    "\n",
    "for eps in eps_list:\n",
    "    show_cluster_samples(data, X, y, eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour tous les clusters nous avons choisi de ne pas afficher les bruits (données pour lesquelles la valeur du \n",
    "cluster est égale à `-1`). En effet, bien qu'elles soient regroupées dans un même ensemble, ces données ne présentent \n",
    "pas de caractéristiques communes mis à part d'être considérées différentes de toutes les autres données par `DBSCAN` \n",
    "\n",
    "### Analyse pour `eps=5`\n",
    "\n",
    "Nous observons ici un seul cluster composé de 4 photos de la même personne : `Junichiro Koizumi`.\n",
    "\n",
    "### Analyse pour `eps=6`\n",
    "\n",
    "Là encore, nous observons le cluster trouvé précédemment mais cette fois-ci, avec une photo de plus de la même personne.\n",
    "\n",
    "### Analyse pour `eps=7`\n",
    "\n",
    "Pour cette valeur d'`eps` nous observons une nette amélioration. En effet, l'algorithme a réussi à identifier 12 \n",
    "groupes de photos. `DBSCAN` semble ici avoir regroupé les photos selon l'orientation du visage. On note tout de même \n",
    "que les clusters sont tous assez petits (6 photos pour le gros cluster). \n",
    "\n",
    "### Analyse pour `eps=8`\n",
    "\n",
    "À partir de cette valeur d'`eps`, il semble que `DBSCAN` ait plus de mal à séparer les visages. En effet, bien qu'on\n",
    "puisse noter la présence de 4 petits clusters (avec 5 photos au maximum), on remarque aussi l'émergence d'un grand\n",
    "cluster (avec 256 photos). De plus, les petits clusters semblent ne plus uniquement regrouper des personnes selon\n",
    "l'orientation du visage mais aussi selon la personne (seul le dernier cluster est composé de deux personnalités\n",
    "différentes). A contrario le premier cluster regroupe beaucoup de personnalités différentes dans des orientations du\n",
    "visage différentes et avec des traits de caractères (comme la présence de rides / cernes, la bouche ouverte...)\n",
    "différents. On peut donc imaginer que la valeur d'`eps` est trop grande et induit de trop grand regroupement.\n",
    "\n",
    "### Analyse pour `eps=9`\n",
    "\n",
    "Les résultats observés précédemment se confirment avec la présence ici d'un cluster encore plus grand (694 images) et\n",
    "d'un petit cluster (3 images) composé uniquement de la personnalité `Jiang Zemin` possédant des traits assez spécifiques\n",
    "(en particulier de très grandes lunettes bien visible).\n",
    "\n",
    "### Analyse pour `eps=10`\n",
    "\n",
    "Légère amélioration ici des résultats avec 2 petits clusters (3 photos chacun) avec une orientation du visage\n",
    "intra-cluster similaire et avec le premier cluster composé de différentes personnalités. En revanche, nous notons aussi\n",
    "que le gros cluster a quasiment doublé de taille.\n",
    "\n",
    "### Analyse pour `eps=11`, `eps=12`, `eps=13`, `eps=14` et `eps=15`\n",
    "\n",
    "Pour toutes ces valeurs d'`eps`, nous n'avons désormais plus qu'un très gros cluster qui croit avec l'augmentation\n",
    "d'`eps`.\n",
    "\n",
    "### Bilan\n",
    "\n",
    "Contrairement à ce que nous montrait le score de silhouette et comme nous l'avions imaginé avec l'analyse du nombre de\n",
    "clusters, les résultats que nous observons ici semblent nous indiquer que de nombreux (même petits) clusters seraient\n",
    "préférables à de gros clusters avec le modèle `DBSCAN`."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
